{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/header.png\" width=\"100%\">\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit III: Probabilistic filtering\n",
    "#### Inferring user intention in a noisy world\n",
    "<b>[John Williamson](http://johnhw.com)</b> \n",
    "\n",
    "----\n",
    "\n",
    "    All theorems are true. \n",
    "    All models are wrong. \n",
    "    And all data are inaccurate. \n",
    "\n",
    "    What are we to do? \n",
    "    We must be sure to remain uncertain.\n",
    "\n",
    "-- *[Leonard A. Smith, Proc. International School of Physics ``Enrico Fermi\", (1997)](http://www2.maths.ox.ac.uk/~lenny/fermi96_main_abs.html)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"imgs/Capture.PNG\"/>\n",
    "*A probabilistic filter-based gesture recogniser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "-----------------\n",
    "\n",
    "### What is probabilistic filtering?\n",
    "One view on interaction is to see user intentions as **unknown values** which are partially observed through input sensors. The time series of inputs from the user only give a partial, noisy, incomplete view of intention inside the user's head. \n",
    "\n",
    "### Interaction model\n",
    "<img src=\"imgs/brainspace.png\" width=\"100%\">\n",
    "\n",
    "#### Probabilistic filtering in HCI\n",
    "Probabilistic filtering **(PF)** tracks the evolution of some unknown variables [user intentions] given observed evidence [user input], in a way that is **robust**. Probabilistic filters infer a **distribution** over possible hidden (unobserved) variables, updating them over time. They are inherently **uncertain** (they represent degrees of belief) and **dynamic** (they explicitly model changing state over time).\n",
    "\n",
    "Probabilistic filtering is an **inverse probability** approach, and it requires that we think of interaction from an unique perspective. We have to explicitly be able to write down:\n",
    "\n",
    "* what we want to know (i.e. the **state space of intention**);\n",
    "* how that will change over time (i.e. the **dynamics of intention**);\n",
    "*  a model that *if we knew what the user intention was, what the expected behavior would be* (i.e. a **function mapping intention -> expected user inputs**).\n",
    "\n",
    "Note that this last point is the **inverse** of the typical way of approaching this problem, where we would try and find a mapping from a sensors to intention, by design or by learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this computational HCI?\n",
    "Probabilistic filtering means writing down an **executable, statistical model** of user behavior, then **running an inference algorithm** that updates beliefs based on the way observations evolve. The **parameters** of the filter can be **learned from data**.\n",
    "\n",
    "This has four key elements of computational interaction:\n",
    "* an explicit mathematical model of user-system behavior;\n",
    "* a way of updating that model with observed data from users;\n",
    "* an algorithmic element that, using this model, can apply computational power to improving interaction;\n",
    "* the ability to simulate or synthesize elements of the expected user-system behavior.\n",
    "\n",
    "It satisfies the requirement that better interfaces can be achieved via:\n",
    "* improved modeling;\n",
    "* better data collection;\n",
    "* more powerful algorithms;  \n",
    "* or increased computational power, \n",
    "\n",
    "rather than the workhorses of traditional HCI:\n",
    "* more design ingenuity;\n",
    "* and stronger evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are competitive approaches?\n",
    "* **Crafted mappings**, where we try to find (by hand) transforms from sensors to intentions that are  simple or obvious. **Example:** a button, which has two physical states, and maps on to two intentional states via two electrical states. Pushed down = current flows = user intended to switch on. The mapping from electrical states to intentional states is **designed.**\n",
    "\n",
    "* **Machine learned mappings**, where we train a system to recognize a class of input patterns as being representative of an intended behavior. **Example:** Finger gesture recognizer; hundreds of examples of many users performing one of N multi-touch gestures are recorded. These are used to train a random forest to classify the intended gesture. The mapping from electrical states (capacitive sensors) to intentional states is **learned**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "* **Robustness to noise** PFs work well even with input sensors that are noisy.\n",
    "* **Robustness to poorly specified models** PFs can cope predictably even if our models are bad.\n",
    "* **Robustness to intermittence** PFs can continue to sensibly interpolate when input cuts out.\n",
    "* **Uncertainty estimates** PFs *know how certain they are* and this can be used in the interaction design.\n",
    "* **Decoupled from real-time** PFs can infer past (smoothing), present (filtering) and future (forecasting).\n",
    "* **Easy fusion of multiple input sensors** PFs are often used to solely to fuse together multiple inputs from different sensors.\n",
    "* **Better feedback** PFs  offer the opportunity to give users rich insight into the process of intention decoding.\n",
    "* **Flexible modeling** PFs can incorporate both fundamental modeling (e.g. physiological or cognitive models) and data-driven machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "* 1960s Kalman filter (Swerling, Kalman, Bucy), Extended Kalman Filter (Schmidt)\n",
    "* late 1960-1990s Particle filter / sequential Monte Carlo\n",
    "* 1992 Bootstrap filter (Gordon)\n",
    "* 1995 Unscented Kalman Filter (Uhlmann)\n",
    "* 1998 Condensation: particle filter for vision problems (Isard and Blake) \n",
    "\n",
    "**We will base our model in this unit roughly on the algorithm variant proposed by Isard and Blake.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles \n",
    "-------\n",
    "### Overview diagram\n",
    "<img src=\"imgs/control_loop.png\">\n",
    "\n",
    "\n",
    "\n",
    "Notation:\n",
    "* We have a sequence of states over time, indexed by $t$\n",
    "* $X_t$ the variable we want to know (at time $t$). \n",
    "* $Y_t$ the variable we can observe.\n",
    "* $\\hat{X_t}$ our estimate of the variable we want to know.\n",
    "\n",
    "* We want to compute $\\hat{X_t}=P(X_t|Y_t)$ (the **inverse problem**). \n",
    "* We use a **forward model** $P(Y_t|X_t)$ to infer this.\n",
    "* We need to define two functions: $Y_t = f(X_t)$ (the **observation function**) and $X_{t} = g(X_{t-1})$ (the **dynamics** or **process function**).\n",
    "\n",
    "* $f$ and $g$ are often very simple functions.\n",
    "\n",
    "<img src=\"imgs/stochastic.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case\n",
    "### Problem description\n",
    "We are going to solve xxx\n",
    "\n",
    "#### Algorithm\n",
    "We will use the **particle filter** algorithm, although I will briefly explain how an unscented Kalman filter could be used for part of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the things we need\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pykalman, pfilter\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 8.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test data\n",
    "To test the particle filter, we will try and track a very simple, 1D sine wave:\n",
    "$$Y_t=\\sin(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0,20,100)\n",
    "x = np.sin(t)\n",
    "plt.plot(t,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We will use a very simple model\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that there are no predictable dynamics, just some Gaussian noise $X_t = X_t+1 + \\epsilon$\n",
    "\n",
    "* **Evidence**\n",
    "We assume that the sensor we measure is the value we want to infer, i.e. $Y_t=X_t$, and $g(X_t)$  is just the identity.\n",
    "\n",
    "* **Weighting**\n",
    "We weight samples according to how similar they are to the observed output. We use a simple RBF kernel:\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\sigma^2}\\right)}$$\n",
    "Note that this gives more weight to particles that are more similar to the observation: it is a **similarity** function, not a distance function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Identity Example\n",
    "def dynamics(x):\n",
    "    # tomorrow is the same as today\n",
    "    # but slightly randomly different\n",
    "    return x+np.random.normal(0,0.1,x.shape)\n",
    "\n",
    "def observe(x):\n",
    "    # we observe x directly\n",
    "    return x[:,0]\n",
    "\n",
    "def weight(true_y, hypothesized_y):\n",
    "    # RBF similarity function\n",
    "    return np.exp(-np.sum((true_y-hypothesized_y)**2, axis=0)/0.05)\n",
    "    \n",
    "from scipy.stats import norm\n",
    "\n",
    "# we assume that, before seeing any evidence, that the particles are \n",
    "# normally distributed about 0, with std. dev. 1.0\n",
    "prior = [norm(0,1)] # x ~ N(0,1)\n",
    "\n",
    "pf_simple = pfilter.ParticleFilter(initial=prior, \n",
    "                                    observe_fn=observe,\n",
    "                                    n_particles=200,                                    \n",
    "                                    dynamics_fn=dynamics,\n",
    "                                    weight_fn=weight,                    \n",
    "                                    resample_proportion=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_filter(pfilter, time, inputs, expected):\n",
    "    \"\"\"Apply a particle filter to a time series, and plot the\n",
    "    first component of the predictions alongside the expected\n",
    "    output.\"\"\"\n",
    "    plt.plot(time, expected, 'C1', lw=3)\n",
    "    particles = []\n",
    "    for i in range(len(x)):    \n",
    "        pfilter.update(np.array([x[i]]).reshape(1,1))\n",
    "        particles.append(pfilter.particles[:,0])    \n",
    "    ts = np.tile(time[:,None], pfilter.n_particles)    \n",
    "    plt.scatter(ts, np.array(particles), alpha=0.2, c='C0', s=1)\n",
    "    plt.legend([\"True\",\"Estimated\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_filter(pf_simple, t, x, x)\n",
    "plt.ylim(-1.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more interesting example\n",
    "Imagine we wanted to infer the **phase** of the oscillator driving this sine wave. The phase variable is not observable, but we want to infer it from the observed oscillation. Furthermore, we want the *unwrapped* phase, i.e. we expect the phase to monotonically increase.\n",
    "\n",
    "* **Evidence**\n",
    "We postulate an observation model:\n",
    "$$Y_t = \\sin(X_t)$$\n",
    "\n",
    "Because we defined $Y_t=\\sin(t)$, we are actually trying to infer $t$.\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that we have a very simple dynamical system, where we have a velocity and a position.\n",
    "$$X_t = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix},$$ and \n",
    "$$X_{t+1} = X_t + \\begin{bmatrix}\\dot{x} \\\\ 0 \\end{bmatrix}.$$\n",
    "\n",
    "* **Priors**\n",
    "We again assume that the initial distribution is normally distributed, but that the std. dev. of the velocity is smaller than the position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Example\n",
    "def linear_dynamics(x):    \n",
    "    y = np.array(x)\n",
    "    y[:,0] += y[:,1]    \n",
    "    \n",
    "    # process noise \n",
    "    y += np.random.normal(0,[0.1,0.001],x.shape)\n",
    "    return y\n",
    "\n",
    "def observe_sin(x):    \n",
    "    # y = sin(x)    \n",
    "    return np.sin(x[:,0])\n",
    "\n",
    "def weight_sin(true_y, hypothesized_y):\n",
    "    return np.exp(-np.sum((true_y-hypothesized_y)**2, axis=0)/0.5)\n",
    "    \n",
    "from scipy.stats import norm\n",
    "prior = [norm(0,0.25), norm(0,0.35)] \n",
    "\n",
    "pf_sin = pfilter.ParticleFilter(initial=prior, \n",
    "                                observe_fn=observe_sin,\n",
    "                                n_particles=200,\n",
    "                                dynamics_fn=linear_dynamics,\n",
    "                                weight_fn=weight_sin,                    \n",
    "                                resample_proportion=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf_sin.init_filter()\n",
    "plot_filter(pf_sin, t, x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key algorithm summary\n",
    "| Algorithm       | Dynamics       | State distribution | Efficiency | Optimizable |\n",
    "|-----------------|----------------|--------------------|------------|-----------|\n",
    "| Particle        | Arbitrary      | Arbitrary          | Low        | No        |\n",
    "| Kalman          | Linear         | Gaussian           | Very high  | Yes       |\n",
    "| Extended Kalman | Locally linear | Gaussian           | High       | Yes       |\n",
    "| Unscented Kalman| Arbitrary      | Gaussian           | High       | ?         |\n",
    "| HMM             | Transitions    | Discrete           | High       | Yes       |\n",
    "\n",
    "* Dynamics: permissible state transition functions (i.e. how we go from now to the next timestep).\n",
    "* State distribution: distribution type for representing current state. Gaussian distributions are very efficient, but can't represent multiple modes.\n",
    "* Efficiency: computational efficiency.\n",
    "* Optimizable: is there an algorithm to optimize the  parameters of the filter be *automatically* given training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gallery\n",
    "Research papers here (thumbnail + link), short description of why cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls\n",
    "Hands-on guru knowledge goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "---------------------\n",
    "### Scope and limitations\n",
    "#### Scope\n",
    "\n",
    "#### Limitations\n",
    "* PFs can be computationally intensive to run. \n",
    "* Curse-of-dimensionality can make the attractive simplicity of PFs work poorly in practice as the state space expands.\n",
    "* Sometimes the inverse probability model can be hard to formulate.\n",
    "* Particle filters are simple and elegant, but inferentially weak.\n",
    "* Kalman filters are rigid and restrictive, but very inferentially efficient.\n",
    "* Hybrid approaches (Ensemble Kalman filter, Unscented Kalman Filter, hybrid particle/Kalman filters) can trade these qualities off, but they aren't off the shelf solutions (i.e. you need an expert!).\n",
    "\n",
    "\n",
    "### Resources\n",
    "#### Basic\n",
    "* Read the [Condensation paper](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/isard-blake-98.pdf).\n",
    "* Read [the Kalman filter in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n",
    "* Watch [the particle filter without equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "\n",
    "#### Advanced\n",
    "* [A technical but succinct and clear explanation of the particle filter](http://www.cns.nyu.edu/~eorhan/notes/particle-filtering.pdf)\n",
    "* [A bibliography of particle filter papers](http://www.stats.ox.ac.uk/~doucet/smc_resources.html)\n",
    "\n",
    "**some more HCI related resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Future of probabilistic filtering\n",
    "\n",
    "#### Learned models\n",
    "\n",
    "Much use of probabilistic filters has depended on strong mathematical models of the fundamental process. For example, in rocket science, sophisticated physics models were used to specify the Kalman filters used for stable control. \n",
    "\n",
    "However, it is becoming increasingly possible to **infer** these models from observations. Techniques such as deep learning (for example variational autoencoders or generative adversarial networks) make it possible to learn very sophisticated *generative models* from observations of\n",
    "data.  \n",
    "\n",
    "These models can be dropped into probabilistic filters to produce robust inferential engines for user interaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
