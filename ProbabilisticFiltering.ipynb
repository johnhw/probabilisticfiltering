{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/header.png\" width=\"100%\">\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit III: Probabilistic filtering\n",
    "#### Inferring user intention in a noisy world\n",
    "<b>[John Williamson](http://johnhw.com)</b> \n",
    "\n",
    "----\n",
    "\n",
    "    All theorems are true. \n",
    "    All models are wrong. \n",
    "    And all data are inaccurate. \n",
    "\n",
    "    What are we to do? \n",
    "    We must be sure to remain uncertain.\n",
    "\n",
    "-- *[Leonard A. Smith, Proc. International School of Physics ``Enrico Fermi\", (1997)](http://www2.maths.ox.ac.uk/~lenny/fermi96_main_abs.html)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "-----------------\n",
    "### What are we going to do?\n",
    "We will:\n",
    "* show how to represent interaction problems as inference;\n",
    "* discuss how probabilistic filters can be used to attack these inference problems;\n",
    "* specifically show how motion-based interfaces can use probabilistic filtering to increase robustness.\n",
    "\n",
    "**The main purpose of this tutorial is to think about interaction problems as inference, and introduce some practical approaches to solve them.**\n",
    "\n",
    "### What will we *practically* do?\n",
    "* We will build a 2D mouse gesture recognizer using a hybrid discrete/continuous particle filter. This will be a simple, robust classifier with rich feedback opportunities.\n",
    "<img  src=\"imgs/Capture.PNG\" width=\"800px\"/>\n",
    "\n",
    "* We will build an algorithm to track finger pose from raw capacitive touch sensor readings using a continuous particle filter. We will find this works well even with a poorly specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#### What is probabilistic filtering ?\n",
    "One view on interaction is to see user intentions as **unknown values** which are partially observed through input sensors. The time series of inputs from the user only give a partial, noisy, incomplete view of intention inside the user's head. \n",
    "\n",
    "Probabilistic filtering **(PF)** tracks the evolution of some unknown variables *[user intentions]* given observed evidence *[user input]*, in a way that is **robust**. Probabilistic filters infer a **distribution** over possible hidden (unobserved) variables, updating them over time. They are inherently **uncertain**, as they represent degrees of belief, and **dynamic**, as they explicitly model changing state over time.\n",
    "\n",
    "<img src=\"imgs/brain_inference.png\">\n",
    "\n",
    "#### Simulation viewpoint\n",
    "These filters are really *simulators*. They *simulate* how possible user behaviours might unfold over time. In some probabilistic filters, hundreds of parallel simulators are run, each with slightly different parameters. In all cases, the simulations are adjusted online to better match observed reality. The internal parameters that drive the simulation are the *unknown variables* we want to infer and the *evidence* is the observed reality that adjusts the simulation parameters.\n",
    "\n",
    "#### Properties\n",
    "Probabilistic filtering is:\n",
    "\n",
    "| Property | Why  |\n",
    "|----------|------|\n",
    "|**Bayesian**  |  Represents degrees of belief using probability distributions    |\n",
    "|**predictive**  |  Works by comparing predictions with reality   |\n",
    "|**generative** |  Involves generating (i.e. simulating) behavior   |\n",
    "\n",
    "-----\n",
    "Probabilistic filtering is an **inverse probability** approach, and it requires that we think of interaction from an unique perspective. We have to explicitly be able to write down:\n",
    "\n",
    "* what we want to know (i.e. the **state space of intention**);\n",
    "* how that will change over time (i.e. the **dynamics of intention**);\n",
    "*  a model that *if we knew what the user intention was, what the expected behavior would be* (i.e. a **generative function mapping intention -> expected user inputs**).\n",
    "\n",
    "Note that this last point is the **inverse** of the typical way of approaching this problem, where we would try and find a mapping from a sensors to intention, by design or by learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this computational HCI?\n",
    "Probabilistic filtering means writing down an **executable, statistical model** of user behavior, then **running an inference algorithm** that updates beliefs based on the way observations evolve. The **parameters** of the filter can be **learned from data**.\n",
    "\n",
    "This has four key elements of computational interaction:\n",
    "* an explicit mathematical model of user-system behavior;\n",
    "* a way of updating that model with observed data from users;\n",
    "* an algorithmic element that, using this model, can apply computational power to improving interaction;\n",
    "* the ability to simulate or synthesize elements of the expected user-system behavior.\n",
    "\n",
    "It satisfies the requirement that better interfaces can be achieved via:\n",
    "* improved modeling;\n",
    "* better data collection;\n",
    "* more powerful algorithms;  \n",
    "* or increased computational power, \n",
    "\n",
    "rather than the workhorses of traditional HCI:\n",
    "* more design ingenuity;\n",
    "* and stronger evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are competitive approaches?\n",
    "#### **Crafted mappings**\n",
    "**where we try to find (by hand) transforms from sensors to intentions that are  simple or obvious.**\n",
    "\n",
    "**Example:** a button, which has two physical states, and maps on to two intentional states via two electrical states. Pushed down = current flows = user intended to switch on. The mapping from electrical states to intentional states is **designed.**\n",
    "<img src=\"imgs/undo.jpg\">\n",
    "*[Image credit: David Singleton via flickr.com CC-BY 2.0]*\n",
    "\n",
    "#### **Machine learned mappings**\n",
    "**where we train a system to recognize a class of input patterns as being representative of an intended behavior. **\n",
    "**Example:** Finger gesture recognizer; hundreds of examples of many users performing one of N multi-touch gestures are recorded. These are used to train a random forest to classify the intended gesture. The mapping from electrical states (capacitive sensors) to intentional states is **learned**.\n",
    "\n",
    "<img src=\"imgs/svm.jpg\" width=\"300px\">\n",
    "*[Image credit: Elisfm - via Wikimedia Commons; public domain]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "* **Robustness to noise** PFs work well even with input sensors that are noisy.\n",
    "* **Robustness to poorly specified models** PFs can cope predictably even if our models are bad.\n",
    "* **Robustness to intermittence** PFs can continue to sensibly interpolate when input cuts out.\n",
    "* **Uncertainty estimates** PFs *know how certain they are* and this can be used in the interaction design.\n",
    "* **Decoupled from real-time** PFs can infer past (smoothing), present (filtering) and future (forecasting).\n",
    "* **Easy fusion of multiple input sensors** PFs are often used to solely to fuse together multiple inputs from different sensors.\n",
    "* **Better feedback** PFs  offer the opportunity to give users rich insight into the process of intention decoding.\n",
    "* **Flexible modeling** PFs can incorporate both fundamental modeling (e.g. physiological or cognitive models) and data-driven machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History\n",
    "* 1960s Kalman filter (Swerling, Kalman, Bucy), Extended Kalman Filter (Schmidt)\n",
    "* late 1960-1990s Particle filter / sequential Monte Carlo\n",
    "* 1992 Bootstrap filter (Gordon)\n",
    "* 1995 Unscented Kalman Filter (Uhlmann)\n",
    "* 1998 Condensation: particle filter for vision problems (Isard and Blake) \n",
    "\n",
    "**We will base our model in this unit roughly on the algorithm variant proposed by Isard and Blake.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles \n",
    "-------\n",
    "\n",
    "> Interaction is the process of driving a system into a state compatible with user intentions.\n",
    "\n",
    "There are many perspectives on interaction from this stance, including:\n",
    "\n",
    "| Perspective   | Burden | Characteristic                         |\n",
    "|---------------|--------|----------------------------------------|\n",
    "| Communication | User   | User gets information into the system. |\n",
    "| Control       | Split  | User drives state towards intention.   |\n",
    "| Inference     | System | Systems infers what user intention is. |\n",
    "\n",
    "### Interaction as inference\n",
    "If we view interaction as inference of intention, there are three elements:\n",
    "* **Interaction is inference**; it is the process of inferring a hidden variable: what the user wants a system to do. \n",
    "* **Observations are noisy and incomplete** What a system sees is a distorted and incomplete representation of user actions in the world, which are in turn a noisy representation of internal intentions (your hand does not always go where you want it...)\n",
    "* **Interaction occurs over time** Interaction is a *process* that evolves over time. Information flow is not instantaneous.\n",
    "\n",
    "<img src=\"imgs/brainspace.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview diagram\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/control_loop.png\">\n",
    "\n",
    "\n",
    "\n",
    "Notation:\n",
    "* We have a sequence of states over time, indexed by $t$\n",
    "* $X_t$ the variable we want to know (at time $t$). \n",
    "* $Y_t$ the variable we can observe.\n",
    "\n",
    "\n",
    "* We want to compute $P(X_t|Y_t)$ (the **inverse problem**). \n",
    "* We use a **forward model** $P(Y_t|X_t)$ to infer this.\n",
    "* We need to define two functions: ${\\bf\\hat{y_t}} = f({\\bf \\hat{x}}_t)$ (the **observation function**) and $\\hat{\\bf x}_{t} = g(\\hat{\\bf x}_{t-1})$ (the **dynamics** or **process function**).\n",
    "\n",
    "* $f$ and $g$ are often very simple functions.\n",
    "\n",
    "<img src=\"imgs/stochastic.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case\n",
    "### Problem description\n",
    "We are going to solve xxx\n",
    "\n",
    "#### Algorithm\n",
    "We will use the **particle filter** algorithm (technically the **SIR** variant, which is the simplest to understand).\n",
    "\n",
    "A particle filter requires that we specify:\n",
    "* A **dynamics function** that predicts how we expect the world to evolve, which takes \n",
    "$\\hat{\\bf{x}}_t \\rightarrow \\hat{\\bf x}_{t+1}$\n",
    "* An **observation function** that predicts what we expect to observe, given a hypothesized state $\\hat{\\bf y}_t = \\hat{\\bf{x}_t}$\n",
    "* A function that, given a hypothesized observation $\\hat{\\bf y}_t$, can be used to compute $p(\\hat{\\bf y}_t|{\\bf y}_t)$. This is performed by computing weights $w_i$ for each particle $i$ using a **weight function**, and then normalizing to produce a probability:\n",
    "$$p^{(i)}(\\hat{\\bf y}_t|{\\bf y}_t) = \\frac{w_i}{\\sum_j w_j}$$\n",
    "* A set of **prior distributions** that specify our initial guesses for $\\hat{\\bf x}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic filtering\n",
    "We will first implement a basic particle filter that can track a very simple one dimensional time series. \n",
    "\n",
    "Then, we will show how this can generalise to an interesting HCI problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the things we need\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pykalman, pfilter\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import matplotlib, matplotlib.colors\n",
    "matplotlib.rcParams['figure.figsize'] = (14.0, 8.0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test data\n",
    "To test the particle filter, we will try and track a very simple, 1D sine wave:\n",
    "$$Y_t=\\sin(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0,20,100).reshape(-1,1)\n",
    "x = np.sin(t)\n",
    "plt.plot(t,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "We will use a very simple model\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that there are no predictable dynamics, just some Gaussian noise $X_t = X_{t+1} + \\epsilon,\\  \\epsilon \\sim N(0,\\sigma_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Identity Example\n",
    "def dynamics(x):\n",
    "    # tomorrow is the same as today\n",
    "    # but slightly randomly different\n",
    "    sigma_p = 0.2\n",
    "    return x+np.random.normal(0,sigma_p,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show what this dynamics model looks like, if we had no resampling step in the filter. This makes predictions without ever having seen any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_dynamics(priors, dynamics, steps, n_runs=1):\n",
    "    runs = []\n",
    "    def simulate_run():\n",
    "        x = np.array([p.rvs() for p in priors])\n",
    "        xs = [x]    \n",
    "        for i in range(steps):\n",
    "            x = dynamics(x)\n",
    "            xs.append(x)\n",
    "        return xs    \n",
    "    return np.array([simulate_run() for j in range(n_runs)])\n",
    "    \n",
    "simulated = simulate_dynamics(priors=[norm(0,1)], dynamics=dynamics, steps=200, n_runs=20)   \n",
    "plt.plot(simulated[:,:,0].T, alpha=0.3, c='C0');\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"X\")\n",
    "plt.title(\"Simulated runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observation**\n",
    "We assume that the sensor we measure is the value we want to infer, i.e. $Y_t=X_t$, and $g(X_t)$  is just the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def observe(x):\n",
    "    # we observe x directly\n",
    "    return x[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Weighting**\n",
    "We weight samples according to how similar they are to the observed output. We use a simple heat kernel:\n",
    "$$w_i = e^{\\left(-\\frac{(y-y^\\prime)^2}{2\\beta^2}\\right)}$$\n",
    "$\\beta$ is a parameter that lets us specify how precise our matching between observation and reality is.\n",
    "\n",
    "Note that this gives more weight to particles that are more similar to the observation: it is a **similarity** function, not a distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight(true_y, hypothesized_y):\n",
    "    # RBF similarity function\n",
    "    beta = 0.25\n",
    "    return np.exp(-np.sum((true_y-hypothesized_y)**2, axis=0)/(0.5*beta**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this function looks like, comparing a true value of 0 with values in [-1, 1], $\\beta=0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hypo = np.linspace(-1,1,100).reshape(1,100)\n",
    "plt.plot(hypo.reshape(100,), weight(0, hypo))\n",
    "plt.axvline(0,c='C1')\n",
    "plt.xlabel(\"Hypothesized value\")\n",
    "plt.ylabel(\"Unnormalised weight\")\n",
    "plt.title(\"RBF kernel, $\\\\beta$=0.25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priors**\n",
    "We assume a very simple prior on $X$; that it is normally distributed, mean 0, variance 1, $X_0 \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# we assume that, before seeing any evidence, that the particles are \n",
    "# normally distributed about 0, with std. dev. 1.0\n",
    "prior = [norm(0,1)] # x ~ N(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Filter creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf_simple = pfilter.ParticleFilter(initial=prior, \n",
    "                                    observe_fn=observe,\n",
    "                                    n_particles=200,                                    \n",
    "                                    dynamics_fn=dynamics,\n",
    "                                    weight_fn=weight,                    \n",
    "                                    resample_proportion=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_pfilter(pfilter, inputs):\n",
    "    \"\"\"Apply a particle filter to a time series of inputs,\n",
    "    and return the particles, their weights, and the mean\n",
    "    estimated state\"\"\"    \n",
    "    # reset filter\n",
    "    pfilter.init_filter()\n",
    "    particles = []\n",
    "    weights = []\n",
    "    means = []    \n",
    "    # apply to each element of the time series\n",
    "    for i in range(len(inputs)):    \n",
    "        pfilter.update([inputs[i]])\n",
    "        particles.append(pfilter.particles)    \n",
    "        weights.append(pfilter.weights)\n",
    "        means.append(pfilter.mean_state)        \n",
    "    return np.array(particles), np.array(weights), np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_pfilter(time,expected, observed, particles, weights, means):\n",
    "    \"\"\"Apply a particle filter to a time series, and plot the\n",
    "    first component of the predictions alongside the expected\n",
    "    output.\"\"\"\n",
    "    # expected output\n",
    "    plt.plot(time, expected, 'C1', lw=3)\n",
    "    plt.plot(time, observed, 'C3', lw=3)\n",
    "    \n",
    "    # particles \n",
    "    ts = np.tile(time[:,None], particles.shape[1]).ravel()\n",
    "    weights =  weights.ravel()    \n",
    "    rgba_colors = np.zeros((len(weights),4))\n",
    "    rgba_colors[:,0:3] = matplotlib.colors.to_rgb('C2')\n",
    "    weights *= 10\n",
    "    rgba_colors[:, 3] = np.clip(weights, 0, 1)\n",
    "    plt.scatter(ts, particles[:,:,0].ravel(),  c=rgba_colors, s=2)\n",
    "    # mean estimation\n",
    "    plt.plot(time, means, 'C0--', lw=2)\n",
    "    # legend\n",
    "    plt.legend([\"True\", \"Observed\", \"Mean estimate\", \"Particle\"])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"X\")\n",
    "    plt.title(\"Particle filter estimate\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0,0.002, x.shape)\n",
    "particles, weights, means = run_pfilter(pf_simple, x+noise)\n",
    "plot_pfilter(t, x, x, particles, weights, means)\n",
    "plt.ylim(-1.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* This is a trivial model, but still tracks \"complex\" functions, because it is adapting to observations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more interesting example\n",
    "Imagine we wanted to infer the **phase** of the oscillator driving this sine wave. The phase variable is not observable, but we want to infer it from the observed oscillation. Furthermore, we want the *unwrapped* phase, i.e. we expect the phase to monotonically increase.\n",
    "\n",
    "We can encode these assumptions in our model, then see if the particle filter is able to infer the hidden parameter over time.\n",
    "\n",
    "* **Observation**\n",
    "We postulate an observation model:\n",
    "$$Y_t = \\sin(X_t)$$\n",
    "i.e. that what we see is the effect of sine on a hidden variable $X_t$.\n",
    "Because we defined $Y_t=\\sin(t)$, we are actually trying to infer $t$.\n",
    "\n",
    "* **Dynamics**\n",
    "We assume that we have a very simple dynamical system in discrete time, where we have a velocity and a position.\n",
    "$$X_t = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix},$$ and \n",
    "$$X_{t+1} = X_t + \\begin{bmatrix}\\dot{x} \\\\ 0 \\end{bmatrix}.$$\n",
    "\n",
    "This means that if we start linearly increasing or decreasing at a certain rate, we should expect to keep doing so.\n",
    "\n",
    "* **Priors**\n",
    "We again assume that the initial distribution is normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Example\n",
    "def linear_dynamics(x):        \n",
    "    nx = np.dot(x,np.array([[1,0],\n",
    "                            [1,1]]))        \n",
    "    process_sigmas = [0.1, 0.001] # how much noise for x and dx    \n",
    "    nx += np.random.normal(0,process_sigmas,x.shape)\n",
    "    return nx\n",
    "\n",
    "def observe_sin(x):    \n",
    "    # y = sin(x)    \n",
    "    return np.sin(x[:,0])\n",
    "\n",
    "def weight_sin(true_y, hypothesized_y):\n",
    "    beta = 0.5 # how like the observations we expect our simulations to be\n",
    "    return np.exp(-np.sum((true_y-hypothesized_y)**2, axis=0)/beta)\n",
    "    \n",
    "from scipy.stats import norm\n",
    "prior = [norm(0,1), norm(0,0.25)] \n",
    "\n",
    "pf_sin = pfilter.ParticleFilter(initial=prior, \n",
    "                                observe_fn=observe_sin,\n",
    "                                n_particles=200,\n",
    "                                dynamics_fn=linear_dynamics,\n",
    "                                weight_fn=weight_sin,                    \n",
    "                                resample_proportion=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "particles, weights, means = run_pfilter(pf_sin, x)\n",
    "plot_pfilter(t, t, x, particles, weights, means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to note\n",
    "* The particle filter was able to infer the hidden state, despite only having a forward model (i.e knowledge of $\\sin(x)$, **not** $\\sin^{-1}(x)$)\n",
    "* It correctly unwrapped phase, because we primed it with the dynamics model to expect values that would increase at a linear rate.\n",
    "* This problem results in *multimodal* distributions, because there are an infinite number of solutions to $y=sin(x)$ because we can add any multiple of $2\\pi$ without changing anything.\n",
    "We can see these as fainter lines on the particle plot.\n",
    "* This means that the particle mean is not actually a good estimate in this case! (we could use the median here instead, but this is harder in higher dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key algorithm summary\n",
    "| Algorithm       | Dynamics       | State distribution | Efficiency | Optimizable |\n",
    "|-----------------|----------------|--------------------|------------|-----------|\n",
    "| Particle        | Arbitrary      | Arbitrary          | Low        | Tricky    |\n",
    "| Kalman          | Linear         | Gaussian           | Very high  | Easy      |\n",
    "| Extended Kalman | Locally linear | Gaussian           | High       | Yes       |\n",
    "| Unscented Kalman| Arbitrary      | Gaussian           | High       | Tricky    |\n",
    "| HMM             | Transitions    | Discrete           | High       | Yes       |\n",
    "\n",
    "* Dynamics: permissible state transition functions (i.e. how we go from now to the next timestep).\n",
    "* State distribution: distribution type for representing current state. Gaussian distributions are very efficient, but can't represent multiple modes.\n",
    "* Efficiency: computational efficiency.\n",
    "* Optimizable: is there an algorithm to optimize the  parameters of the filter be *automatically* given training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gallery\n",
    "Research papers here (thumbnail + link), short description of why cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls\n",
    "Hands-on guru knowledge goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "---------------------\n",
    "### Scope and limitations\n",
    "#### Scope\n",
    "\n",
    "#### Limitations\n",
    "* PFs can be computationally intensive to run. \n",
    "* Curse-of-dimensionality can make the attractive simplicity of PFs work poorly in practice as the state space expands.\n",
    "* Sometimes the inverse probability model can be hard to formulate.\n",
    "* Particle filters are simple and elegant, but inferentially weak.\n",
    "* Kalman filters are rigid and restrictive, but very inferentially efficient.\n",
    "* Hybrid approaches (Ensemble Kalman filter, Unscented Kalman Filter, hybrid particle/Kalman filters) can trade these qualities off, but they aren't off the shelf solutions (i.e. you need an expert!).\n",
    "\n",
    "\n",
    "### Resources\n",
    "#### Basic\n",
    "* Read the [Condensation paper](http://vision.stanford.edu/teaching/cs231b_spring1415/papers/isard-blake-98.pdf).\n",
    "* Read [the Kalman filter in pictures](http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n",
    "* Watch [the particle filter without equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "\n",
    "#### Advanced\n",
    "* [A technical but succinct and clear explanation of the particle filter](http://www.cns.nyu.edu/~eorhan/notes/particle-filtering.pdf)\n",
    "* [A bibliography of particle filter papers](http://www.stats.ox.ac.uk/~doucet/smc_resources.html)\n",
    "\n",
    "**some more HCI related resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Future of probabilistic filtering\n",
    "\n",
    "#### Learned models\n",
    "\n",
    "Much use of probabilistic filters has depended on strong mathematical models of the fundamental process. For example, in rocket science, sophisticated physics models were used to specify the Kalman filters used for stable control. \n",
    "\n",
    "However, it is becoming increasingly possible to **infer** these models from observations. Techniques such as deep learning (for example variational autoencoders or generative adversarial networks) make it possible to learn very sophisticated *generative models* from observations of\n",
    "data.  \n",
    "\n",
    "These models can be dropped into probabilistic filters to produce robust inferential engines for user interaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
